{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit.Chem import rdDistGeom\n",
    "class Graph:\n",
    "    def __init__(self, molecule_smiles:str, node_vec_len:int, max_atoms:int=None):\n",
    "        self.smiles = molecule_smiles\n",
    "        self.node_vec_len = node_vec_len\n",
    "        self.max_atoms = max_atoms\n",
    "        self.smiles_to_mol()\n",
    "        if self.mol is not None:\n",
    "            self.smiles_to_graph()\n",
    "\n",
    "    def smiles_to_mol(self):\n",
    "        mol = Chem.MolFromSmiles(self.smiles)\n",
    "        if mol is None:\n",
    "            self.mol = None\n",
    "        self.mol =Chem.AddHs(mol)\n",
    "\n",
    "    def smiles_to_graph(self):\n",
    "        atoms = self.mol.GetAtoms()\n",
    "\n",
    "        if self.max_atoms is None:\n",
    "            n_atoms = len(list(atoms))\n",
    "        else:\n",
    "            n_atoms = self.max_atoms\n",
    "        \n",
    "        node_mat = np.zeros((n_atoms, self.node_vec_len))\n",
    "\n",
    "        for atom in atoms:\n",
    "            atom_idx = atom.GetIdx()\n",
    "            atom_no = atom.GetAtomicNum()\n",
    "            node_mat[atom_idx, atom_no]= 1\n",
    "          \n",
    "\n",
    "        adj_mat = rdmolops.GetAdjacencyMatrix(self.mol)\n",
    "        self.std_adj_mat = np.copy(adj_mat)\n",
    "\n",
    "        dist_mat = rdDistGeom.GetDistanceMatrix(self.mol)\n",
    "        dist_mat[dist_mat == 0.] = 1\n",
    "\n",
    "        adj_mat = adj_mat *(1/dist_mat)\n",
    "\n",
    "        dim_add = n_atoms - adj_mat.shape[0]\n",
    "        adj_mat = np.pad(adj_mat, pad_width= ((0, dim_add), (0, dim_add)), mode= 'constant')\n",
    "        adj_mat = adj_mat + np.eye(n_atoms)\n",
    "\n",
    "        self.node_mat = node_mat\n",
    "        self.adj_mat = adj_mat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "class GraphData(Dataset):\n",
    "    def __init__(self, dataset_path:str, node_vec_len:int, max_atoms:int=None):\n",
    "        self.node_vec_len = node_vec_len\n",
    "        self.max_atoms = max_atoms\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        self.indices = df.index.to_list()\n",
    "        self.smiles = df['smiles'].to_list()\n",
    "        self.outputs = df['measured log solubility in mols per litre'].to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        smile = self.smiles[idx]\n",
    "        graph = Graph(smile, self.node_vec_len, self.max_atoms)\n",
    "        node_mat = torch.Tensor(graph.node_mat)\n",
    "        adj_mat = torch.Tensor(graph.adj_mat)\n",
    "        output = torch.Tensor([self.outputs[idx]])\n",
    "        return (node_mat, adj_mat), output , smile\n",
    "\n",
    "\n",
    "def collate_graph_dataset(dataset: Dataset):\n",
    "    node_mats = []\n",
    "    adj_mats = []\n",
    "    outputs = []\n",
    "    smiles = []\n",
    "    for (node_mat, adj_mat), output, smile in dataset:\n",
    "        node_mats.append(node_mat)\n",
    "        adj_mats.append(adj_mat)\n",
    "        outputs.append(output)\n",
    "        smiles.append(smile)\n",
    "    \n",
    "    node_mats_tensor = torch.cat(node_mats, dim=0)\n",
    "    adj_mats_tensor = torch.cat(adj_mats, dim=0)\n",
    "    outputs_tensor = torch.cat(outputs, dim=0)\n",
    "\n",
    "    return (node_mats_tensor, adj_mats_tensor), outputs_tensor, smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConvolutionLayer(nn.Module):\n",
    "    def __inin__(self, node_in_len, node_out_len):\n",
    "        super(ConvolutionLayer, self).__init__()\n",
    "        self.conv_linear = nn.Linear(node_in_len, node_out_len)\n",
    "        self.con_activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, node_mat, adj_mat):\n",
    "        n_neighbors = adj_mat.sum(dim= -1, keepdim= True)\n",
    "        self.idx_mat = torch.eye(adj_mat.shape[-2], adj_mat.shape[-1], device=n_neighbors.device)\n",
    "        idx_mat = self.ids_mat.unsqueeze(0).expand(*adj_mat.shape)\n",
    "        inv_deg_mat = torch.mul(idx_mat, 1/n_neighbors)\n",
    "        node_feats = torch.bmm(inv_deg_mat,adj_mat)\n",
    "        node_feats = torch.bmm(node_feats, node_mat)\n",
    "        node_feats = self.conv_linear(node_feats)\n",
    "        node_feats = self.con_activation(node_feats)\n",
    "        return node_feats\n",
    "\n",
    "\n",
    "class PoolingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoolingLayer, self).__init__()\n",
    "\n",
    "    def forward(self, node_feats):\n",
    "        pooled_node_feats = node_feats.mean(dim=1)\n",
    "        return pooled_node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemGCN(nn.Module):\n",
    "    def __init__(self, node_vec_len, node_feats_len, hiden_feats_len,n_cov, n_hidden,n_outputs, p_dropout=0.0):\n",
    "        super(ChemGCN, self).__init__()\n",
    "        self.init_transorm = nn.Linear(node_vec_len,node_feats_len)\n",
    "        self.conv_layers = nn.ModuleList([ConvolutionLayer(node_feats_len, node_feats_len) for _ in range(n_cov)])\n",
    "        self.pooling = PoolingLayer()\n",
    "        pooled_node_feats_len = node_feats_len\n",
    "        self.pooled_activation = nn.LeakyReLU()\n",
    "        self.pooled_to_hidden = nn.Linear(pooled_node_feats_len, hiden_feats_len)\n",
    "        self.hidden_layer = nn.Linear(hiden_feats_len, hiden_feats_len)\n",
    "        self.hidden_activation = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.n_hidden = n_hidden\n",
    "        if self.n_hidden > 1:\n",
    "            self.hidden_layers = nn.ModuleList([self.hiddenlayer for _ in range(n_hidden-1)])\n",
    "            self.hidden_activation_layers = nn.ModuleList([self.hidden_activation for _ in range(n_hidden-1)])\n",
    "            self.dropouts = nn.ModuleList([self.dropout for _ in range(n_hidden-1)])\n",
    "        self.hidden_to_output = nn.Linear(hiden_feats_len, n_outputs)\n",
    "\n",
    "    def forward(self, node_mat, adj_mat):\n",
    "        node_feats = self.init_transorm(node_mat)\n",
    "        for conv_layer in self.conv_layers:\n",
    "            node_feats = conv_layer(node_feats, adj_mat)\n",
    "        pooled_node_feats = self.pooling(node_feats)\n",
    "        pooled_node_feats = self.pooled_activation(pooled_node_feats)\n",
    "\n",
    "        pooled_node_feats = self.pooled_to_hidden(pooled_node_feats)\n",
    "        pooled_node_feats = self.hidden_activation(pooled_node_feats)\n",
    "        pooled_node_feats = self.dropout(pooled_node_feats)\n",
    "\n",
    "        if self.n_hidden > 1:\n",
    "            for hidden_layer, hidden_activation, dropout in zip(self.hidden_layers, self.hidden_activation_layers, self.dropouts):\n",
    "                pooled_node_feats = hidden_layer(pooled_node_feats)\n",
    "                pooled_node_feats = hidden_activation(pooled_node_feats)\n",
    "                pooled_node_feats = dropout(pooled_node_feats)\n",
    "        output = self.hidden_to_output(pooled_node_feats)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer:\n",
    "    def __init__(self, x):\n",
    "        self.mean = torch.mean(x)\n",
    "        self.std = torch.std(x)\n",
    "    def standarize(self, x):\n",
    "        z = (x - self.mean)/self.std\n",
    "        return z\n",
    "    def restore(self, z):\n",
    "        x = z * self.std + self.mean\n",
    "        return x\n",
    "    def state(self):\n",
    "        return {'mean': self.mean, 'std': self.std}\n",
    "        \n",
    "    def load(self, state):\n",
    "        self.mean = state['mean']\n",
    "        self.std = state['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "def train_model(epoch, model, train_loader, optimizer, loss_func, standardizer,max_atoms,node_vec_len, device):\n",
    "    avg_loss = 0\n",
    "    avg_mea = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for i, dataset in enumerate(train_loader):\n",
    "        node_mat = dataset[0][0]\n",
    "        adj_mat = dataset[0][1]\n",
    "        output = dataset[1]\n",
    "\n",
    "        first_dim = int(torch.numel(node_mat)/(max_atoms*node_vec_len))\n",
    "        node_mat = node_mat.view(first_dim, max_atoms, node_vec_len)\n",
    "        adj_mat = adj_mat.view(first_dim, max_atoms, max_atoms)\n",
    "\n",
    "        node_mat = standardizer.standarize(node_mat)\n",
    "        adj_mat = standardizer.standarize(adj_mat)\n",
    "\n",
    "        output_std = standardizer.standarize(output)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        nn_preds = model(node_mat.to(device), adj_mat.to(device))\n",
    "        loss = loss_func(nn_preds, output_std.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        prediction = standardizer.restore(nn_preds.detach().cpu())\n",
    "        mae = mean_absolute_error(prediction, output) \n",
    "        avg_mae += mae\n",
    "        avg_loss += loss\n",
    "        count += 1\n",
    "    avg_loss /= count\n",
    "    avg_mae /= count\n",
    "    print(f\"Epoch: {epoch}, Loss: {avg_loss}, MAE: {avg_mae}\")\n",
    "    return avg_loss, avg_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual.seed(0)\n",
    "\n",
    "max_atoms = 200\n",
    "node_vec_len = 60\n",
    "train_size = 0.7\n",
    "batch_size = 32\n",
    "hidden_nodes = 60\n",
    "n_conv_layers = 4\n",
    "n_hidden_layers = 2\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "main_path = Path(__file__).resolve().parent\n",
    "dataset_path = main_path / 'data' / 'solubility.csv'\n",
    "dataset = GraphData(dataset_path, node_vec_len, max_atoms)\n",
    "dataset_indices = np.arange(0,len(dataset), 1)\n",
    "train_size = int(np.round(train_size*len(dataset)))\n",
    "test_size = len(dataset) - train_size\n",
    "train_indices = np.random.choice(dataset_indices, train_size, replace=False)\n",
    "test_indices = np.array(list(set(dataset_indices) - set(train_indices)))\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn = collate_graph_dataset)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, collate_fn = collate_graph_dataset)\n",
    "\n",
    "\n",
    "model = ChemGCN(node_vec_len,hidden_nodes, hidden_nodes, n_conv_layers, n_hidden_layers, n_outputs=1, p_dropout=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "outputs = [dataset[i][1] for i in range(len(dataset))]\n",
    "standardizer = Standardizer(torch.Tensor(outputs))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "loss = []\n",
    "mae = []\n",
    "epoch = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    l, m = train_model(i, model, train_loader, optimizer, loss_func, standardizer, max_atoms, node_vec_len, device)\n",
    "    loss.append(l)\n",
    "    mae.append(m)\n",
    "    epoch.append(i)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss, test_mae = model(model, test_loader, loss_func, standardizer, max_atoms, node_vec_len, device)\n",
    "    print(f'Training loss: {loss[-1]:.4f}, Training MAE: {mae[-1]:.4f}')\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('pyg_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55dd076cc796caf2ac139f6c8599c1e42cd7645ba998294b8b9abf123fd2eabd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
